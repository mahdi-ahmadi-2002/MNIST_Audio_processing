{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1788543,"sourceType":"datasetVersion","datasetId":1063054}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-29T07:35:02.222924Z","iopub.execute_input":"2024-01-29T07:35:02.223230Z","iopub.status.idle":"2024-01-29T07:35:03.056416Z","shell.execute_reply.started":"2024-01-29T07:35:02.223204Z","shell.execute_reply":"2024-01-29T07:35:03.055668Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport librosa\nimport scipy.io.wavfile as wav\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:35:03.058242Z","iopub.execute_input":"2024-01-29T07:35:03.058735Z","iopub.status.idle":"2024-01-29T07:35:07.058000Z","shell.execute_reply.started":"2024-01-29T07:35:03.058702Z","shell.execute_reply":"2024-01-29T07:35:07.057040Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchaudio","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:35:07.059133Z","iopub.execute_input":"2024-01-29T07:35:07.059527Z","iopub.status.idle":"2024-01-29T07:35:07.200474Z","shell.execute_reply.started":"2024-01-29T07:35:07.059502Z","shell.execute_reply":"2024-01-29T07:35:07.199753Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"root = \"/kaggle/input/audio-mnist/data\"\ndata = []\ntarget = []\nperson = []\n\nfor i in range(1, 61):\n    folderPath = os.path.join(root,str(i).zfill(2))\n    files = os.listdir(folderPath)\n    for file in files:\n        tempPerson = ''\n        temp = librosa.load(folderPath + '/' + file)\n        temp = temp[0]\n        target += file[0]\n        person.append(int(i))\n        data.append(temp)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:35:07.202086Z","iopub.execute_input":"2024-01-29T07:35:07.202355Z","iopub.status.idle":"2024-01-29T07:38:27.555698Z","shell.execute_reply.started":"2024-01-29T07:35:07.202331Z","shell.execute_reply":"2024-01-29T07:38:27.554797Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(target).value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:38:47.530641Z","iopub.execute_input":"2024-01-29T07:38:47.531010Z","iopub.status.idle":"2024-01-29T07:38:47.544187Z","shell.execute_reply.started":"2024-01-29T07:38:47.530984Z","shell.execute_reply":"2024-01-29T07:38:47.543309Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0    3000\n1    3000\n2    3000\n3    3000\n4    3000\n5    3000\n6    3000\n7    3000\n8    3000\n9    3000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"class_names=sorted(pd.DataFrame(target)[0].unique().tolist())","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:38:48.440689Z","iopub.execute_input":"2024-01-29T07:38:48.441051Z","iopub.status.idle":"2024-01-29T07:38:48.450585Z","shell.execute_reply.started":"2024-01-29T07:38:48.441021Z","shell.execute_reply":"2024-01-29T07:38:48.449512Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\npaddedDataa = pad_sequences(data, padding='post', dtype='float32')","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:38:49.033635Z","iopub.execute_input":"2024-01-29T07:38:49.033971Z","iopub.status.idle":"2024-01-29T07:39:00.717329Z","shell.execute_reply.started":"2024-01-29T07:38:49.033945Z","shell.execute_reply":"2024-01-29T07:39:00.715852Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"paddedDataa.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:39:00.719064Z","iopub.execute_input":"2024-01-29T07:39:00.719998Z","iopub.status.idle":"2024-01-29T07:39:00.736094Z","shell.execute_reply.started":"2024-01-29T07:39:00.719934Z","shell.execute_reply":"2024-01-29T07:39:00.734738Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(30000, 22050)"},"metadata":{}}]},{"cell_type":"code","source":"target = [int(item) for item in target]","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:39:00.737823Z","iopub.execute_input":"2024-01-29T07:39:00.738237Z","iopub.status.idle":"2024-01-29T07:39:00.772115Z","shell.execute_reply.started":"2024-01-29T07:39:00.738192Z","shell.execute_reply":"2024-01-29T07:39:00.770807Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport scipy.io.wavfile as wav\nimport librosa\nfrom torch.nn.functional import pad\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\nclass AudioMNISTDataset(Dataset):\n    def __init__(self, X, y):\n        self.ToSpectrogram = torchaudio.transforms.MelSpectrogram()\n        self.ToDB = torchaudio.transforms.AmplitudeToDB()\n        paddedDataa = pad_sequences(X, padding='post', dtype='float32')\n        self.audio = paddedDataa\n        self.labels = y\n        assert(len(self.audio) == len(self.labels))\n    \n    def __len__(self):\n        return len(self.audio)\n    \n    def padding(self, file):\n        audio_padded = torch.zeros((1,25500))\n        audio_padded[0,:len(file)] = torch.Tensor(file)\n        return audio_padded\n    \n    def mfcc_data(self, file):\n#         print(file)\n\n        spectrogram = self.ToSpectrogram(file)\n        spectrogram = self.ToDB(spectrogram)\n        return spectrogram[0]\n    \n    def __getitem__(self, idx):\n        padded = self.padding(self.audio[idx])\n#         print(padded.size())\n#         print(self.audio[idx])\n#         audio_seq = self.mfcc_data(torch.tensor(self.audio[idx]))\n        audio_seq = self.mfcc_data(padded)\n#         print(audio_seq.size())\n#         print(audio_seq)\n        normalized_data = (audio_seq - (-100)) / (20 - (-100))\n#         print(normalized_data.shape)\n        normalized_data = torch.FloatTensor(normalized_data)\n\n        label = self.labels[idx]\n#         plt.imshow(normalized_data)\n#         print(normalized_data.shape)\n#         normalized_data = normalized_data.reshape(16384,)\n#         print(normalized_data.shape)\n\n#         normalized_data = normalized_data.permute(1, 2, 0)\n        return normalized_data.flatten(), label","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:49.599567Z","iopub.execute_input":"2024-01-29T07:47:49.599935Z","iopub.status.idle":"2024-01-29T07:47:49.611693Z","shell.execute_reply.started":"2024-01-29T07:47:49.599906Z","shell.execute_reply":"2024-01-29T07:47:49.610523Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:49.798264Z","iopub.execute_input":"2024-01-29T07:47:49.798585Z","iopub.status.idle":"2024-01-29T07:47:49.802864Z","shell.execute_reply.started":"2024-01-29T07:47:49.798558Z","shell.execute_reply":"2024-01-29T07:47:49.801868Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(paddedDataa, target, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:49.961663Z","iopub.execute_input":"2024-01-29T07:47:49.961938Z","iopub.status.idle":"2024-01-29T07:47:50.732735Z","shell.execute_reply.started":"2024-01-29T07:47:49.961914Z","shell.execute_reply":"2024-01-29T07:47:50.731940Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\ntrain_loader = DataLoader(AudioMNISTDataset(X_train, y_train), batch_size=32, shuffle=True)\ntest_loader = DataLoader(AudioMNISTDataset(X_test, y_test), batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:50.734324Z","iopub.execute_input":"2024-01-29T07:47:50.734627Z","iopub.status.idle":"2024-01-29T07:47:51.969533Z","shell.execute_reply.started":"2024-01-29T07:47:50.734600Z","shell.execute_reply":"2024-01-29T07:47:51.968514Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# for images, labels in train_loader:\n#     print(type(images))    \n#     print(images.shape)\n#     print(images.max())\n#     break\nx, y = next(iter(train_loader))\nx.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:51.970857Z","iopub.execute_input":"2024-01-29T07:47:51.971634Z","iopub.status.idle":"2024-01-29T07:47:52.006206Z","shell.execute_reply.started":"2024-01-29T07:47:51.971597Z","shell.execute_reply":"2024-01-29T07:47:52.005346Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 16384]), torch.Size([32]))"},"metadata":{}}]},{"cell_type":"code","source":"print(y)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:52.007753Z","iopub.execute_input":"2024-01-29T07:47:52.008023Z","iopub.status.idle":"2024-01-29T07:47:52.016327Z","shell.execute_reply.started":"2024-01-29T07:47:52.008000Z","shell.execute_reply":"2024-01-29T07:47:52.015452Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"tensor([6, 2, 8, 9, 4, 5, 3, 2, 5, 4, 5, 5, 8, 7, 6, 9, 9, 7, 0, 3, 6, 8, 4, 7,\n        8, 7, 1, 4, 9, 2, 5, 0])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:55.255480Z","iopub.execute_input":"2024-01-29T07:47:55.256310Z","iopub.status.idle":"2024-01-29T07:47:55.260923Z","shell.execute_reply.started":"2024-01-29T07:47:55.256276Z","shell.execute_reply":"2024-01-29T07:47:55.259914Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nimport matplotlib.pyplot as plt\n\nclass VoiceMNISTTransformer(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes, num_heads, num_layers):\n        super(VoiceMNISTTransformer, self).__init__()\n\n        self.embedding = nn.Linear(input_dim, hidden_dim)\n#         self.transformer = nn.TransformerEncoder(\n#             nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads),\n#             num_layers=num_layers\n#         )\n        self.transformer = nn.Transformer(d_model=hidden_dim,\n                                          num_encoder_layers=num_layers,\n                                          num_decoder_layers=num_layers,\n                                          nhead=num_heads,\n                                          batch_first=True).encoder\n        self.fc = nn.Linear(hidden_dim, num_classes)\n        self.bn = nn.LazyBatchNorm1d()\n\n    def forward(self, x):\n#         print(x.shape)\n        x = self.bn(self.embedding(x)).relu()\n#         x = x.view(x.size(0), x.size(1), -1)  # Reshape to [sequence_length, batch_size, feature_dim]\n#         x = x.permute(1, 0, 2)\n#         print(x.shape)\n        output = self.transformer(x)\n#         output = output.mean(dim=0)# Aggregate across time steps\n#         print(output.shape)\n        output = self.fc(output)\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:55.452544Z","iopub.execute_input":"2024-01-29T07:47:55.453148Z","iopub.status.idle":"2024-01-29T07:47:55.461835Z","shell.execute_reply.started":"2024-01-29T07:47:55.453118Z","shell.execute_reply":"2024-01-29T07:47:55.460918Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:21.712239Z","iopub.execute_input":"2024-01-29T07:47:21.712612Z","iopub.status.idle":"2024-01-29T07:47:21.717165Z","shell.execute_reply.started":"2024-01-29T07:47:21.712583Z","shell.execute_reply":"2024-01-29T07:47:21.716066Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:47:23.480170Z","iopub.execute_input":"2024-01-29T07:47:23.480623Z","iopub.status.idle":"2024-01-29T07:47:23.486750Z","shell.execute_reply.started":"2024-01-29T07:47:23.480587Z","shell.execute_reply":"2024-01-29T07:47:23.485799Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VoiceMNISTTransformer(16384, 1024, 10, 8, 6).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:01:27.557877Z","iopub.execute_input":"2024-01-28T15:01:27.558593Z","iopub.status.idle":"2024-01-28T15:01:28.836225Z","shell.execute_reply.started":"2024-01-28T15:01:27.558559Z","shell.execute_reply":"2024-01-28T15:01:28.835080Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"model(x.to(device)).shape","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:01:28.838364Z","iopub.execute_input":"2024-01-28T15:01:28.838932Z","iopub.status.idle":"2024-01-28T15:01:28.857494Z","shell.execute_reply.started":"2024-01-28T15:01:28.838889Z","shell.execute_reply":"2024-01-28T15:01:28.856506Z"},"trusted":true},"execution_count":140,"outputs":[{"execution_count":140,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 10])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming your spectrogram images have a shape (batch_size, num_channels, height, width)\ninput_dim = 16384  # Adjust based on the actual width of your spectrogram\nhidden_dim = 512\nnum_classes = 10  # Number of classes (digits 0 to 9)\nnum_heads = 8\nnum_layers = 4\nnum_epochs = 10\nmodel = VoiceMNISTTransformer(input_dim, hidden_dim, num_classes, num_heads, num_layers).to(device)\nstart_time = time.time()\ntrain_losses = []\ntest_losses = []\ntrain_accuracy = []\ntest_accuracy = []\nnum = []\n# Define your loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\ncounter = 0\n# Now you can use the trained model for prediction\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    \n    trn_corr = 0\n    for batch_idx, (X_train, y_train) in enumerate(train_loader):\n        optimizer.zero_grad()\n        y_pred = model(X_train.to(device))\n        loss = criterion(y_pred, y_train.to(device))\n        loss.backward()\n        optimizer.step()\n        counter += 1\n        predicted = torch.max(y_pred.data, 1)[1]\n        batch_corr = (predicted == y_train.to(device)).sum()\n        trn_corr += batch_corr\n\n#         if batch_idx % 100 == 0:\n#         if batch_idx % 100 == 1:\n        if batch_idx != 0:\n#             print(f\"Training Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.6f}\")\n            train_accuracy.append(trn_corr.item() * 100 / ((32 * batch_idx)+1))\n            num.append(counter/len(train_loader))\n            train_losses.append(loss.item())\n\n#     train_loss = loss.item()\n#     train_acc = trn_corr.item() * 100 / (32 * (batch_idx + 1))\n#     train_losses.append(train_loss)\n#     train_accuracy.append(train_acc)\n\n#     Validation/Test phase\n            if batch_idx % 100 == 1:\n\n                model.eval()\n                tst_corr = 0\n                all_true = []\n                all_pred = []\n\n                with torch.no_grad():\n                    for batch_idx, (X_test, y_test) in enumerate(test_loader):\n                        X_test = X_test.to(device)\n                        y_test = y_test.to(device)\n                        y_val = model(X_test)\n                        loss = criterion(y_val, y_test)\n\n                        predicted = torch.max(y_val.data, 1)[1]\n                        batch_corr = (predicted == y_test).sum()\n                        tst_corr += batch_corr\n\n                        all_true.extend(y_test.cpu().numpy())\n                        all_pred.extend(predicted.cpu().numpy())\n\n                test_loss = loss.item()\n                test_acc = tst_corr.item() * 100 / len(test_loader.dataset)\n                test_losses.append(test_loss)\n                test_accuracy.append(test_acc)\n\n                # Compute and print F1 score, accuracy, precision, and recall\n                f1 = f1_score(all_true, all_pred, average='weighted')\n                acc = accuracy_score(all_true, all_pred)\n                precision = precision_score(all_true, all_pred, average='weighted')\n                recall = recall_score(all_true, all_pred, average='weighted')\n\n                print(f\"\\nValidation/Test Loss: {test_loss:.6f}, Accuracy: {test_acc:.2f}%\")\n                print(f\"F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-29T07:51:18.257423Z","iopub.execute_input":"2024-01-29T07:51:18.258222Z","iopub.status.idle":"2024-01-29T08:00:07.947496Z","shell.execute_reply.started":"2024-01-29T07:51:18.258187Z","shell.execute_reply":"2024-01-29T08:00:07.946641Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\nValidation/Test Loss: 2.367240, Accuracy: 15.11%\nF1 Score: 0.0867, Precision: 0.1088, Recall: 0.1511\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\nValidation/Test Loss: 1.871769, Accuracy: 31.36%\nF1 Score: 0.2265, Precision: 0.3350, Recall: 0.3136\n\nValidation/Test Loss: 0.633748, Accuracy: 78.89%\nF1 Score: 0.7791, Precision: 0.8325, Recall: 0.7889\n\nValidation/Test Loss: 0.211220, Accuracy: 89.62%\nF1 Score: 0.8926, Precision: 0.9048, Recall: 0.8962\n\nValidation/Test Loss: 0.118471, Accuracy: 93.98%\nF1 Score: 0.9398, Precision: 0.9415, Recall: 0.9398\n\nValidation/Test Loss: 0.092610, Accuracy: 93.62%\nF1 Score: 0.9360, Precision: 0.9416, Recall: 0.9362\n\nValidation/Test Loss: 0.064987, Accuracy: 94.96%\nF1 Score: 0.9496, Precision: 0.9518, Recall: 0.9496\n\nValidation/Test Loss: 0.177325, Accuracy: 92.38%\nF1 Score: 0.9256, Precision: 0.9354, Recall: 0.9238\n\nEpoch 2/10\n\nValidation/Test Loss: 0.108227, Accuracy: 90.42%\nF1 Score: 0.9037, Precision: 0.9176, Recall: 0.9042\n\nValidation/Test Loss: 0.036675, Accuracy: 95.47%\nF1 Score: 0.9548, Precision: 0.9567, Recall: 0.9547\n\nValidation/Test Loss: 0.018500, Accuracy: 96.69%\nF1 Score: 0.9668, Precision: 0.9672, Recall: 0.9669\n\nValidation/Test Loss: 0.050936, Accuracy: 96.09%\nF1 Score: 0.9609, Precision: 0.9617, Recall: 0.9609\n\nValidation/Test Loss: 0.015351, Accuracy: 95.89%\nF1 Score: 0.9587, Precision: 0.9609, Recall: 0.9589\n\nValidation/Test Loss: 0.030724, Accuracy: 96.64%\nF1 Score: 0.9665, Precision: 0.9675, Recall: 0.9664\n\nValidation/Test Loss: 0.090153, Accuracy: 95.27%\nF1 Score: 0.9520, Precision: 0.9542, Recall: 0.9527\n\nValidation/Test Loss: 0.021587, Accuracy: 95.47%\nF1 Score: 0.9548, Precision: 0.9572, Recall: 0.9547\n\nEpoch 3/10\n\nValidation/Test Loss: 0.011808, Accuracy: 96.33%\nF1 Score: 0.9635, Precision: 0.9656, Recall: 0.9633\n\nValidation/Test Loss: 0.002474, Accuracy: 97.69%\nF1 Score: 0.9769, Precision: 0.9769, Recall: 0.9769\n\nValidation/Test Loss: 0.003461, Accuracy: 97.02%\nF1 Score: 0.9702, Precision: 0.9707, Recall: 0.9702\n\nValidation/Test Loss: 0.044235, Accuracy: 97.22%\nF1 Score: 0.9723, Precision: 0.9728, Recall: 0.9722\n\nValidation/Test Loss: 0.001053, Accuracy: 98.29%\nF1 Score: 0.9829, Precision: 0.9831, Recall: 0.9829\n\nValidation/Test Loss: 0.004321, Accuracy: 97.36%\nF1 Score: 0.9736, Precision: 0.9740, Recall: 0.9736\n\nValidation/Test Loss: 0.001189, Accuracy: 97.87%\nF1 Score: 0.9786, Precision: 0.9793, Recall: 0.9787\n\nValidation/Test Loss: 0.001536, Accuracy: 97.56%\nF1 Score: 0.9756, Precision: 0.9759, Recall: 0.9756\n\nEpoch 4/10\n\nValidation/Test Loss: 0.017441, Accuracy: 97.80%\nF1 Score: 0.9780, Precision: 0.9784, Recall: 0.9780\n\nValidation/Test Loss: 0.002086, Accuracy: 97.78%\nF1 Score: 0.9778, Precision: 0.9779, Recall: 0.9778\n\nValidation/Test Loss: 0.002911, Accuracy: 98.51%\nF1 Score: 0.9851, Precision: 0.9853, Recall: 0.9851\n\nValidation/Test Loss: 0.148648, Accuracy: 97.69%\nF1 Score: 0.9770, Precision: 0.9775, Recall: 0.9769\n\nValidation/Test Loss: 0.001278, Accuracy: 98.53%\nF1 Score: 0.9853, Precision: 0.9854, Recall: 0.9853\n\nValidation/Test Loss: 0.002514, Accuracy: 98.44%\nF1 Score: 0.9845, Precision: 0.9847, Recall: 0.9844\n\nValidation/Test Loss: 0.003523, Accuracy: 97.98%\nF1 Score: 0.9798, Precision: 0.9801, Recall: 0.9798\n\nValidation/Test Loss: 0.034978, Accuracy: 97.76%\nF1 Score: 0.9776, Precision: 0.9781, Recall: 0.9776\n\nEpoch 5/10\n\nValidation/Test Loss: 0.007217, Accuracy: 98.71%\nF1 Score: 0.9871, Precision: 0.9873, Recall: 0.9871\n\nValidation/Test Loss: 0.000741, Accuracy: 98.07%\nF1 Score: 0.9806, Precision: 0.9810, Recall: 0.9807\n\nValidation/Test Loss: 0.000843, Accuracy: 98.16%\nF1 Score: 0.9816, Precision: 0.9818, Recall: 0.9816\n\nValidation/Test Loss: 0.003286, Accuracy: 97.93%\nF1 Score: 0.9794, Precision: 0.9796, Recall: 0.9793\n\nValidation/Test Loss: 0.004284, Accuracy: 98.76%\nF1 Score: 0.9876, Precision: 0.9877, Recall: 0.9876\n\nValidation/Test Loss: 0.000819, Accuracy: 97.67%\nF1 Score: 0.9766, Precision: 0.9771, Recall: 0.9767\n\nValidation/Test Loss: 0.001086, Accuracy: 98.40%\nF1 Score: 0.9840, Precision: 0.9842, Recall: 0.9840\n\nValidation/Test Loss: 0.002388, Accuracy: 98.73%\nF1 Score: 0.9874, Precision: 0.9875, Recall: 0.9873\n\nEpoch 6/10\n\nValidation/Test Loss: 0.001268, Accuracy: 98.40%\nF1 Score: 0.9840, Precision: 0.9843, Recall: 0.9840\n\nValidation/Test Loss: 0.006342, Accuracy: 98.49%\nF1 Score: 0.9849, Precision: 0.9851, Recall: 0.9849\n\nValidation/Test Loss: 0.001491, Accuracy: 98.00%\nF1 Score: 0.9800, Precision: 0.9802, Recall: 0.9800\n\nValidation/Test Loss: 0.013507, Accuracy: 98.11%\nF1 Score: 0.9812, Precision: 0.9815, Recall: 0.9811\n\nValidation/Test Loss: 0.004843, Accuracy: 98.44%\nF1 Score: 0.9844, Precision: 0.9846, Recall: 0.9844\n\nValidation/Test Loss: 0.000608, Accuracy: 98.87%\nF1 Score: 0.9887, Precision: 0.9887, Recall: 0.9887\n\nValidation/Test Loss: 0.001324, Accuracy: 98.69%\nF1 Score: 0.9869, Precision: 0.9870, Recall: 0.9869\n\nValidation/Test Loss: 0.001418, Accuracy: 98.78%\nF1 Score: 0.9878, Precision: 0.9879, Recall: 0.9878\n\nEpoch 7/10\n\nValidation/Test Loss: 0.001320, Accuracy: 98.47%\nF1 Score: 0.9847, Precision: 0.9849, Recall: 0.9847\n\nValidation/Test Loss: 0.000460, Accuracy: 98.64%\nF1 Score: 0.9864, Precision: 0.9865, Recall: 0.9864\n\nValidation/Test Loss: 0.000500, Accuracy: 98.80%\nF1 Score: 0.9880, Precision: 0.9881, Recall: 0.9880\n\nValidation/Test Loss: 0.000151, Accuracy: 98.76%\nF1 Score: 0.9876, Precision: 0.9876, Recall: 0.9876\n\nValidation/Test Loss: 0.000445, Accuracy: 98.53%\nF1 Score: 0.9853, Precision: 0.9854, Recall: 0.9853\n\nValidation/Test Loss: 0.001098, Accuracy: 98.69%\nF1 Score: 0.9869, Precision: 0.9869, Recall: 0.9869\n\nValidation/Test Loss: 0.000340, Accuracy: 99.00%\nF1 Score: 0.9900, Precision: 0.9900, Recall: 0.9900\n\nValidation/Test Loss: 0.001234, Accuracy: 99.02%\nF1 Score: 0.9902, Precision: 0.9903, Recall: 0.9902\n\nEpoch 8/10\n\nValidation/Test Loss: 0.000959, Accuracy: 98.82%\nF1 Score: 0.9882, Precision: 0.9883, Recall: 0.9882\n\nValidation/Test Loss: 0.003491, Accuracy: 99.22%\nF1 Score: 0.9922, Precision: 0.9922, Recall: 0.9922\n\nValidation/Test Loss: 0.001044, Accuracy: 98.80%\nF1 Score: 0.9880, Precision: 0.9881, Recall: 0.9880\n\nValidation/Test Loss: 0.000403, Accuracy: 98.93%\nF1 Score: 0.9893, Precision: 0.9894, Recall: 0.9893\n\nValidation/Test Loss: 0.000561, Accuracy: 98.87%\nF1 Score: 0.9887, Precision: 0.9887, Recall: 0.9887\n\nValidation/Test Loss: 0.000283, Accuracy: 99.09%\nF1 Score: 0.9909, Precision: 0.9909, Recall: 0.9909\n\nValidation/Test Loss: 0.001738, Accuracy: 99.11%\nF1 Score: 0.9911, Precision: 0.9911, Recall: 0.9911\n\nValidation/Test Loss: 0.000293, Accuracy: 98.87%\nF1 Score: 0.9887, Precision: 0.9888, Recall: 0.9887\n\nEpoch 9/10\n\nValidation/Test Loss: 0.000310, Accuracy: 98.87%\nF1 Score: 0.9886, Precision: 0.9888, Recall: 0.9887\n\nValidation/Test Loss: 0.000146, Accuracy: 99.16%\nF1 Score: 0.9916, Precision: 0.9916, Recall: 0.9916\n\nValidation/Test Loss: 0.000149, Accuracy: 99.22%\nF1 Score: 0.9922, Precision: 0.9922, Recall: 0.9922\n\nValidation/Test Loss: 0.001675, Accuracy: 99.02%\nF1 Score: 0.9902, Precision: 0.9903, Recall: 0.9902\n\nValidation/Test Loss: 0.000304, Accuracy: 99.00%\nF1 Score: 0.9900, Precision: 0.9900, Recall: 0.9900\n\nValidation/Test Loss: 0.000201, Accuracy: 99.18%\nF1 Score: 0.9918, Precision: 0.9919, Recall: 0.9918\n\nValidation/Test Loss: 0.002007, Accuracy: 99.02%\nF1 Score: 0.9902, Precision: 0.9903, Recall: 0.9902\n\nValidation/Test Loss: 0.001670, Accuracy: 98.82%\nF1 Score: 0.9882, Precision: 0.9883, Recall: 0.9882\n\nEpoch 10/10\n\nValidation/Test Loss: 0.001565, Accuracy: 98.91%\nF1 Score: 0.9891, Precision: 0.9892, Recall: 0.9891\n\nValidation/Test Loss: 0.000455, Accuracy: 99.02%\nF1 Score: 0.9902, Precision: 0.9903, Recall: 0.9902\n\nValidation/Test Loss: 0.000340, Accuracy: 99.16%\nF1 Score: 0.9916, Precision: 0.9916, Recall: 0.9916\n\nValidation/Test Loss: 0.000162, Accuracy: 99.20%\nF1 Score: 0.9920, Precision: 0.9921, Recall: 0.9920\n\nValidation/Test Loss: 0.001213, Accuracy: 99.13%\nF1 Score: 0.9913, Precision: 0.9913, Recall: 0.9913\n\nValidation/Test Loss: 0.000219, Accuracy: 98.84%\nF1 Score: 0.9884, Precision: 0.9885, Recall: 0.9884\n\nValidation/Test Loss: 0.000185, Accuracy: 98.87%\nF1 Score: 0.9887, Precision: 0.9888, Recall: 0.9887\n\nValidation/Test Loss: 0.002337, Accuracy: 98.36%\nF1 Score: 0.9836, Precision: 0.9838, Recall: 0.9836\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(train_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T08:07:05.840237Z","iopub.execute_input":"2024-01-29T08:07:05.841005Z","iopub.status.idle":"2024-01-29T08:07:06.174615Z","shell.execute_reply.started":"2024-01-29T08:07:05.840969Z","shell.execute_reply":"2024-01-29T08:07:06.173608Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7c86310ac2b0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Plotting\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracy, label='Training Accuracy')\nplt.plot(test_accuracy, label='Test Accuracy')\nplt.title('Accuracy Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f'\\nTotal Duration: {time.time() - start_time:.0f} seconds')","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:43:36.996680Z","iopub.status.idle":"2024-01-28T09:43:36.998610Z","shell.execute_reply.started":"2024-01-28T09:43:36.998335Z","shell.execute_reply":"2024-01-28T09:43:36.998368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F\n\n# Assuming you have a dataset class (CustomDataset) for your images\n\n# Define the transformer model for image classification\nclass TransformerClassifier(nn.Module):\n    def __init__(self, input_size, num_classes, d_model=128, nhead=8, num_layers=6):\n        super(TransformerClassifier, self).__init__()\n        self.bn = nn.LazyBatchNorm1d()\n\n        self.embedding = nn.Linear(input_size, d_model)\n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers,\n        )\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.bn(self.embedding(x)).relu()\n        x = x.permute(1, 0, 2)  # Change the shape for the transformer input\n\n        # Create a fixed positional encoding as a target sequence\n        src = tgt = x\n\n        output = self.transformer(src, tgt)\n        output = output.mean(dim=0)  # Aggregate across sequence length\n        output = self.fc(output)\n        return output\n\n# Hyperparameters\ninput_size = 128 * 128  # Assuming grayscale images\nnum_classes = 10\nbatch_size = 32\nlearning_rate = 0.001\nnum_epochs = 10\n\n# Data loading and preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert image to PyTorch tensor\n    # Add other transformations as needed (e.g., normalization, data augmentation)\n])\n\n\nmodel = TransformerClassifier(input_size=input_size, num_classes=num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for (batch, labels) in train_loader:\n        inputs = batch  # Assuming the data loader returns batches of images\n#         labels = torch.randint(0, num_classes, (batch_size,))\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        predicted = torch.max(y_pred.data, 1)[1]\n        batch_corr = (predicted == y_train.to(device)).sum()\n        trn_corr += batch_corr\n\n#         if batch_idx % 100 == 0:\n#         if batch_idx % 100 == 1:\n        print(f\"Training Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.6f}\")\n        train_accuracy.append(trn_corr.item() * 100 / ((32 * batch_idx) + 1))\n        train_losses.append(loss.item())\n            \n    # Evaluation on the test set\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = batch  # Assuming the data loader returns batches of images\n            labels = torch.randint(0, num_classes, (batch_size,))\n            \n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = correct / total\n    print(f'Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy}')\n\n# Save the trained model if needed\ntorch.save(model.state_dict(), 'transformer_classifier.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T17:55:50.143765Z","iopub.execute_input":"2024-01-28T17:55:50.144136Z","iopub.status.idle":"2024-01-28T17:55:50.342741Z","shell.execute_reply.started":"2024-01-28T17:55:50.144108Z","shell.execute_reply":"2024-01-28T17:55:50.341423Z"},"trusted":true},"execution_count":36,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m model \u001b[38;5;241m=\u001b[39m ImageTransformer(embed_size, num_heads, hidden_dim, num_classes)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Example forward pass\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[36], line 23\u001b[0m, in \u001b[0;36mImageTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the input image\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add a sequence dimension\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x49152 and 16384x256)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (32x49152 and 16384x256)","output_type":"error"}]},{"cell_type":"code","source":"device = 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:00:35.045354Z","iopub.execute_input":"2024-01-28T18:00:35.046205Z","iopub.status.idle":"2024-01-28T18:00:35.050683Z","shell.execute_reply.started":"2024-01-28T18:00:35.046163Z","shell.execute_reply":"2024-01-28T18:00:35.049589Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"embed_size = 256\nnum_heads = 4\nhidden_dim = 512\nnum_classes = 10\ntrain_accuracy = []\ntrain_losses = []\nmodel = ImageTransformer(embed_size, num_heads, hidden_dim, num_classes)\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    trn_corr = 0\n    for batch_idx, (X_train, y_train) in enumerate(train_loader):\n        optimizer.zero_grad()\n        y_pred = model(X_train.to(device))\n        loss = criterion(y_pred, y_train.to(device))\n        loss.backward()\n        optimizer.step()\n\n        predicted = torch.max(y_pred.data, 1)[1]\n        batch_corr = (predicted == y_train.to(device)).sum()\n        trn_corr += batch_corr\n\n#         if batch_idx % 100 == 0:\n#         if batch_idx % 100 == 1:\n        print(f\"Training Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.6f}\")\n        train_accuracy.append(trn_corr.item() * 100 / ((32 * batch_idx)+1))\n        train_losses.append(loss.item())\n\n#     train_loss = loss.item()\n#     train_acc = trn_corr.item() * 100 / (32 * (batch_idx + 1))\n#     train_losses.append(train_loss)\n#     train_accuracy.append(train_acc)\n\n    # Validation/Test phase\n#             model.eval()\n#             tst_corr = 0\n#             all_true = []\n#             all_pred = []\n\n#             with torch.no_grad():\n#                 for batch_idx, (X_test, y_test) in enumerate(test_loader):\n#                     X_test = X_test.to(device)\n#                     y_test = y_test.to(device)\n#                     y_val = model(X_test)\n#                     loss = criterion(y_val, y_test)\n\n#                     predicted = torch.max(y_val.data, 1)[1]\n#                     batch_corr = (predicted == y_test).sum()\n#                     tst_corr += batch_corr\n\n#                     all_true.extend(y_test.cpu().numpy())\n#                     all_pred.extend(predicted.cpu().numpy())\n\n#             test_loss = loss.item()\n#             test_acc = tst_corr.item() * 100 / len(test_loader.dataset)\n#             test_losses.append(test_loss)\n#             test_accuracy.append(test_acc)\n\n#             # Compute and print F1 score, accuracy, precision, and recall\n#             f1 = f1_score(all_true, all_pred, average='weighted')\n#             acc = accuracy_score(all_true, all_pred)\n#             precision = precision_score(all_true, all_pred, average='weighted')\n#             recall = recall_score(all_true, all_pred, average='weighted')\n\n#             print(f\"\\nValidation/Test Loss: {test_loss:.6f}, Accuracy: {test_acc:.2f}%\")\n#             print(f\"F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:07:13.406307Z","iopub.execute_input":"2024-01-28T18:07:13.407011Z","iopub.status.idle":"2024-01-28T18:07:53.390088Z","shell.execute_reply.started":"2024-01-28T18:07:13.406980Z","shell.execute_reply":"2024-01-28T18:07:53.388541Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"\nEpoch 1/10\nTraining Batch 0/797 Loss: 2.411335\nTraining Batch 1/797 Loss: 2.491004\nTraining Batch 2/797 Loss: 2.422049\nTraining Batch 3/797 Loss: 2.328957\nTraining Batch 4/797 Loss: 2.209886\nTraining Batch 5/797 Loss: 2.402220\nTraining Batch 6/797 Loss: 2.341486\nTraining Batch 7/797 Loss: 2.524942\nTraining Batch 8/797 Loss: 2.496173\nTraining Batch 9/797 Loss: 2.238711\nTraining Batch 10/797 Loss: 2.387328\nTraining Batch 11/797 Loss: 2.383668\nTraining Batch 12/797 Loss: 2.457969\nTraining Batch 13/797 Loss: 2.246213\nTraining Batch 14/797 Loss: 2.362269\nTraining Batch 15/797 Loss: 2.597957\nTraining Batch 16/797 Loss: 2.418842\nTraining Batch 17/797 Loss: 2.557679\nTraining Batch 18/797 Loss: 2.325555\nTraining Batch 19/797 Loss: 2.416922\nTraining Batch 20/797 Loss: 2.579286\nTraining Batch 21/797 Loss: 2.370956\nTraining Batch 22/797 Loss: 2.430666\nTraining Batch 23/797 Loss: 2.469038\nTraining Batch 24/797 Loss: 2.404148\nTraining Batch 25/797 Loss: 2.532107\nTraining Batch 26/797 Loss: 2.262216\nTraining Batch 27/797 Loss: 2.393621\nTraining Batch 28/797 Loss: 2.356488\nTraining Batch 29/797 Loss: 2.390170\nTraining Batch 30/797 Loss: 2.441687\nTraining Batch 31/797 Loss: 2.214425\nTraining Batch 32/797 Loss: 2.464738\nTraining Batch 33/797 Loss: 2.392368\nTraining Batch 34/797 Loss: 2.451145\nTraining Batch 35/797 Loss: 2.555535\nTraining Batch 36/797 Loss: 2.612048\nTraining Batch 37/797 Loss: 2.369443\nTraining Batch 38/797 Loss: 2.330876\nTraining Batch 39/797 Loss: 2.292024\nTraining Batch 40/797 Loss: 2.501612\nTraining Batch 41/797 Loss: 2.340591\nTraining Batch 42/797 Loss: 2.322291\nTraining Batch 43/797 Loss: 2.317246\nTraining Batch 44/797 Loss: 2.519184\nTraining Batch 45/797 Loss: 2.205922\nTraining Batch 46/797 Loss: 2.420160\nTraining Batch 47/797 Loss: 2.409346\nTraining Batch 48/797 Loss: 2.572029\nTraining Batch 49/797 Loss: 2.548173\nTraining Batch 50/797 Loss: 2.283736\nTraining Batch 51/797 Loss: 2.418722\nTraining Batch 52/797 Loss: 2.503364\nTraining Batch 53/797 Loss: 2.270879\nTraining Batch 54/797 Loss: 2.368130\nTraining Batch 55/797 Loss: 2.333187\nTraining Batch 56/797 Loss: 2.406133\nTraining Batch 57/797 Loss: 2.666190\nTraining Batch 58/797 Loss: 2.517443\nTraining Batch 59/797 Loss: 2.423180\nTraining Batch 60/797 Loss: 2.456268\nTraining Batch 61/797 Loss: 2.434563\nTraining Batch 62/797 Loss: 2.457749\nTraining Batch 63/797 Loss: 2.511055\nTraining Batch 64/797 Loss: 2.344408\nTraining Batch 65/797 Loss: 2.356702\nTraining Batch 66/797 Loss: 2.408124\nTraining Batch 67/797 Loss: 2.520267\nTraining Batch 68/797 Loss: 2.476164\nTraining Batch 69/797 Loss: 2.459146\nTraining Batch 70/797 Loss: 2.424012\nTraining Batch 71/797 Loss: 2.349750\nTraining Batch 72/797 Loss: 2.352205\nTraining Batch 73/797 Loss: 2.255436\nTraining Batch 74/797 Loss: 2.345166\nTraining Batch 75/797 Loss: 2.537935\nTraining Batch 76/797 Loss: 2.366395\nTraining Batch 77/797 Loss: 2.352408\nTraining Batch 78/797 Loss: 2.437168\nTraining Batch 79/797 Loss: 2.351104\nTraining Batch 80/797 Loss: 2.416672\nTraining Batch 81/797 Loss: 2.466381\nTraining Batch 82/797 Loss: 2.293221\nTraining Batch 83/797 Loss: 2.467387\nTraining Batch 84/797 Loss: 2.373366\nTraining Batch 85/797 Loss: 2.420385\nTraining Batch 86/797 Loss: 2.395916\nTraining Batch 87/797 Loss: 2.315562\nTraining Batch 88/797 Loss: 2.493164\nTraining Batch 89/797 Loss: 2.384302\nTraining Batch 90/797 Loss: 2.389825\nTraining Batch 91/797 Loss: 2.375091\nTraining Batch 92/797 Loss: 2.442078\nTraining Batch 93/797 Loss: 2.430571\nTraining Batch 94/797 Loss: 2.395958\nTraining Batch 95/797 Loss: 2.490426\nTraining Batch 96/797 Loss: 2.481378\nTraining Batch 97/797 Loss: 2.569359\nTraining Batch 98/797 Loss: 2.424284\nTraining Batch 99/797 Loss: 2.354019\nTraining Batch 100/797 Loss: 2.282881\nTraining Batch 101/797 Loss: 2.334266\nTraining Batch 102/797 Loss: 2.447349\nTraining Batch 103/797 Loss: 2.429958\nTraining Batch 104/797 Loss: 2.585402\nTraining Batch 105/797 Loss: 2.335227\nTraining Batch 106/797 Loss: 2.431969\nTraining Batch 107/797 Loss: 2.074118\nTraining Batch 108/797 Loss: 2.312655\nTraining Batch 109/797 Loss: 2.196863\nTraining Batch 110/797 Loss: 2.277978\nTraining Batch 111/797 Loss: 2.425957\nTraining Batch 112/797 Loss: 2.391371\nTraining Batch 113/797 Loss: 2.331451\nTraining Batch 114/797 Loss: 2.324244\nTraining Batch 115/797 Loss: 2.290694\nTraining Batch 116/797 Loss: 2.420738\nTraining Batch 117/797 Loss: 2.569781\nTraining Batch 118/797 Loss: 2.617386\nTraining Batch 119/797 Loss: 2.513968\nTraining Batch 120/797 Loss: 2.264302\nTraining Batch 121/797 Loss: 2.296150\nTraining Batch 122/797 Loss: 2.566706\nTraining Batch 123/797 Loss: 2.415005\nTraining Batch 124/797 Loss: 2.381319\nTraining Batch 125/797 Loss: 2.368435\nTraining Batch 126/797 Loss: 2.511898\nTraining Batch 127/797 Loss: 2.414610\nTraining Batch 128/797 Loss: 2.444774\nTraining Batch 129/797 Loss: 2.409903\nTraining Batch 130/797 Loss: 2.332216\nTraining Batch 131/797 Loss: 2.535786\nTraining Batch 132/797 Loss: 2.392606\nTraining Batch 133/797 Loss: 2.447037\nTraining Batch 134/797 Loss: 2.462613\nTraining Batch 135/797 Loss: 2.506410\nTraining Batch 136/797 Loss: 2.442360\nTraining Batch 137/797 Loss: 2.375675\nTraining Batch 138/797 Loss: 2.387270\nTraining Batch 139/797 Loss: 2.345237\nTraining Batch 140/797 Loss: 2.413385\nTraining Batch 141/797 Loss: 2.425615\nTraining Batch 142/797 Loss: 2.364034\nTraining Batch 143/797 Loss: 2.540815\nTraining Batch 144/797 Loss: 2.312844\nTraining Batch 145/797 Loss: 2.423300\nTraining Batch 146/797 Loss: 2.434124\nTraining Batch 147/797 Loss: 2.586666\nTraining Batch 148/797 Loss: 2.456524\nTraining Batch 149/797 Loss: 2.349713\nTraining Batch 150/797 Loss: 2.374181\nTraining Batch 151/797 Loss: 2.467606\nTraining Batch 152/797 Loss: 2.538443\nTraining Batch 153/797 Loss: 2.356143\nTraining Batch 154/797 Loss: 2.212215\nTraining Batch 155/797 Loss: 2.316964\nTraining Batch 156/797 Loss: 2.324384\nTraining Batch 157/797 Loss: 2.418282\nTraining Batch 158/797 Loss: 2.308017\nTraining Batch 159/797 Loss: 2.484780\nTraining Batch 160/797 Loss: 2.380069\nTraining Batch 161/797 Loss: 2.396316\nTraining Batch 162/797 Loss: 2.373447\nTraining Batch 163/797 Loss: 2.366188\nTraining Batch 164/797 Loss: 2.594493\nTraining Batch 165/797 Loss: 2.422050\nTraining Batch 166/797 Loss: 2.520882\nTraining Batch 167/797 Loss: 2.445856\nTraining Batch 168/797 Loss: 2.445183\nTraining Batch 169/797 Loss: 2.395282\nTraining Batch 170/797 Loss: 2.334620\nTraining Batch 171/797 Loss: 2.320092\nTraining Batch 172/797 Loss: 2.473215\nTraining Batch 173/797 Loss: 2.237282\nTraining Batch 174/797 Loss: 2.420762\nTraining Batch 175/797 Loss: 2.403894\nTraining Batch 176/797 Loss: 2.448153\nTraining Batch 177/797 Loss: 2.429142\nTraining Batch 178/797 Loss: 2.575129\nTraining Batch 179/797 Loss: 2.466543\nTraining Batch 180/797 Loss: 2.358220\nTraining Batch 181/797 Loss: 2.269140\nTraining Batch 182/797 Loss: 2.332439\nTraining Batch 183/797 Loss: 2.304790\nTraining Batch 184/797 Loss: 2.381682\nTraining Batch 185/797 Loss: 2.375346\nTraining Batch 186/797 Loss: 2.438625\nTraining Batch 187/797 Loss: 2.530061\nTraining Batch 188/797 Loss: 2.242625\nTraining Batch 189/797 Loss: 2.326736\nTraining Batch 190/797 Loss: 2.526960\nTraining Batch 191/797 Loss: 2.488804\nTraining Batch 192/797 Loss: 2.492315\nTraining Batch 193/797 Loss: 2.422453\nTraining Batch 194/797 Loss: 2.571926\nTraining Batch 195/797 Loss: 2.489412\nTraining Batch 196/797 Loss: 2.447587\nTraining Batch 197/797 Loss: 2.497452\nTraining Batch 198/797 Loss: 2.415715\nTraining Batch 199/797 Loss: 2.576654\nTraining Batch 200/797 Loss: 2.601039\nTraining Batch 201/797 Loss: 2.455453\nTraining Batch 202/797 Loss: 2.410986\nTraining Batch 203/797 Loss: 2.327532\nTraining Batch 204/797 Loss: 2.326469\nTraining Batch 205/797 Loss: 2.369467\nTraining Batch 206/797 Loss: 2.410700\nTraining Batch 207/797 Loss: 2.512196\nTraining Batch 208/797 Loss: 2.506969\nTraining Batch 209/797 Loss: 2.273899\nTraining Batch 210/797 Loss: 2.347706\nTraining Batch 211/797 Loss: 2.454082\nTraining Batch 212/797 Loss: 2.478747\nTraining Batch 213/797 Loss: 2.400399\nTraining Batch 214/797 Loss: 2.430141\nTraining Batch 215/797 Loss: 2.521863\nTraining Batch 216/797 Loss: 2.355292\nTraining Batch 217/797 Loss: 2.415235\nTraining Batch 218/797 Loss: 2.459566\nTraining Batch 219/797 Loss: 2.464733\nTraining Batch 220/797 Loss: 2.583216\nTraining Batch 221/797 Loss: 2.477198\nTraining Batch 222/797 Loss: 2.394076\nTraining Batch 223/797 Loss: 2.654025\nTraining Batch 224/797 Loss: 2.521626\nTraining Batch 225/797 Loss: 2.331546\nTraining Batch 226/797 Loss: 2.386190\nTraining Batch 227/797 Loss: 2.281700\nTraining Batch 228/797 Loss: 2.417734\nTraining Batch 229/797 Loss: 2.510023\nTraining Batch 230/797 Loss: 2.350667\nTraining Batch 231/797 Loss: 2.356601\nTraining Batch 232/797 Loss: 2.484966\nTraining Batch 233/797 Loss: 2.361002\nTraining Batch 234/797 Loss: 2.514343\nTraining Batch 235/797 Loss: 2.532240\nTraining Batch 236/797 Loss: 2.342658\nTraining Batch 237/797 Loss: 2.387380\nTraining Batch 238/797 Loss: 2.674921\nTraining Batch 239/797 Loss: 2.459677\nTraining Batch 240/797 Loss: 2.454483\nTraining Batch 241/797 Loss: 2.482846\nTraining Batch 242/797 Loss: 2.647918\nTraining Batch 243/797 Loss: 2.409586\nTraining Batch 244/797 Loss: 2.380358\nTraining Batch 245/797 Loss: 2.332886\nTraining Batch 246/797 Loss: 2.212867\nTraining Batch 247/797 Loss: 2.384751\nTraining Batch 248/797 Loss: 2.317541\nTraining Batch 249/797 Loss: 2.358206\nTraining Batch 250/797 Loss: 2.451180\nTraining Batch 251/797 Loss: 2.447627\nTraining Batch 252/797 Loss: 2.275612\nTraining Batch 253/797 Loss: 2.484283\nTraining Batch 254/797 Loss: 2.394164\nTraining Batch 255/797 Loss: 2.531566\nTraining Batch 256/797 Loss: 2.364532\nTraining Batch 257/797 Loss: 2.452774\nTraining Batch 258/797 Loss: 2.388848\nTraining Batch 259/797 Loss: 2.321205\nTraining Batch 260/797 Loss: 2.373517\nTraining Batch 261/797 Loss: 2.420970\nTraining Batch 262/797 Loss: 2.376819\nTraining Batch 263/797 Loss: 2.387678\nTraining Batch 264/797 Loss: 2.378640\nTraining Batch 265/797 Loss: 2.441529\nTraining Batch 266/797 Loss: 2.456979\nTraining Batch 267/797 Loss: 2.307863\nTraining Batch 268/797 Loss: 2.354675\nTraining Batch 269/797 Loss: 2.438275\nTraining Batch 270/797 Loss: 2.432082\nTraining Batch 271/797 Loss: 2.332937\nTraining Batch 272/797 Loss: 2.416214\nTraining Batch 273/797 Loss: 2.353633\nTraining Batch 274/797 Loss: 2.409535\nTraining Batch 275/797 Loss: 2.278487\nTraining Batch 276/797 Loss: 2.425824\nTraining Batch 277/797 Loss: 2.435394\nTraining Batch 278/797 Loss: 2.280812\nTraining Batch 279/797 Loss: 2.393696\nTraining Batch 280/797 Loss: 2.513854\nTraining Batch 281/797 Loss: 2.354584\nTraining Batch 282/797 Loss: 2.461541\nTraining Batch 283/797 Loss: 2.418146\nTraining Batch 284/797 Loss: 2.331892\nTraining Batch 285/797 Loss: 2.435840\nTraining Batch 286/797 Loss: 2.239982\nTraining Batch 287/797 Loss: 2.470267\nTraining Batch 288/797 Loss: 2.417235\nTraining Batch 289/797 Loss: 2.466732\nTraining Batch 290/797 Loss: 2.600260\nTraining Batch 291/797 Loss: 2.371944\nTraining Batch 292/797 Loss: 2.486120\nTraining Batch 293/797 Loss: 2.365819\nTraining Batch 294/797 Loss: 2.292589\nTraining Batch 295/797 Loss: 2.519625\nTraining Batch 296/797 Loss: 2.469344\nTraining Batch 297/797 Loss: 2.385682\nTraining Batch 298/797 Loss: 2.394248\nTraining Batch 299/797 Loss: 2.453902\nTraining Batch 300/797 Loss: 2.412757\nTraining Batch 301/797 Loss: 2.329270\nTraining Batch 302/797 Loss: 2.425690\nTraining Batch 303/797 Loss: 2.470468\nTraining Batch 304/797 Loss: 2.457497\nTraining Batch 305/797 Loss: 2.548056\nTraining Batch 306/797 Loss: 2.235373\nTraining Batch 307/797 Loss: 2.351123\nTraining Batch 308/797 Loss: 2.506806\nTraining Batch 309/797 Loss: 2.388139\nTraining Batch 310/797 Loss: 2.372946\nTraining Batch 311/797 Loss: 2.339601\nTraining Batch 312/797 Loss: 2.244032\nTraining Batch 313/797 Loss: 2.421880\nTraining Batch 314/797 Loss: 2.285939\nTraining Batch 315/797 Loss: 2.583578\nTraining Batch 316/797 Loss: 2.553442\nTraining Batch 317/797 Loss: 2.312307\nTraining Batch 318/797 Loss: 2.472092\nTraining Batch 319/797 Loss: 2.314581\nTraining Batch 320/797 Loss: 2.463438\nTraining Batch 321/797 Loss: 2.301428\nTraining Batch 322/797 Loss: 2.421561\nTraining Batch 323/797 Loss: 2.383707\nTraining Batch 324/797 Loss: 2.417202\nTraining Batch 325/797 Loss: 2.585609\nTraining Batch 326/797 Loss: 2.474140\nTraining Batch 327/797 Loss: 2.337142\nTraining Batch 328/797 Loss: 2.394621\nTraining Batch 329/797 Loss: 2.487781\nTraining Batch 330/797 Loss: 2.433811\nTraining Batch 331/797 Loss: 2.329405\nTraining Batch 332/797 Loss: 2.367978\nTraining Batch 333/797 Loss: 2.352322\nTraining Batch 334/797 Loss: 2.430845\nTraining Batch 335/797 Loss: 2.456232\nTraining Batch 336/797 Loss: 2.482357\nTraining Batch 337/797 Loss: 2.525686\nTraining Batch 338/797 Loss: 2.527334\nTraining Batch 339/797 Loss: 2.456482\nTraining Batch 340/797 Loss: 2.384375\nTraining Batch 341/797 Loss: 2.187872\nTraining Batch 342/797 Loss: 2.375738\nTraining Batch 343/797 Loss: 2.504702\nTraining Batch 344/797 Loss: 2.390480\nTraining Batch 345/797 Loss: 2.426921\nTraining Batch 346/797 Loss: 2.475421\nTraining Batch 347/797 Loss: 2.476763\nTraining Batch 348/797 Loss: 2.509611\nTraining Batch 349/797 Loss: 2.467250\nTraining Batch 350/797 Loss: 2.492873\nTraining Batch 351/797 Loss: 2.426513\nTraining Batch 352/797 Loss: 2.323102\nTraining Batch 353/797 Loss: 2.407408\nTraining Batch 354/797 Loss: 2.337535\nTraining Batch 355/797 Loss: 2.342928\nTraining Batch 356/797 Loss: 2.403297\nTraining Batch 357/797 Loss: 2.267977\nTraining Batch 358/797 Loss: 2.415125\nTraining Batch 359/797 Loss: 2.421306\nTraining Batch 360/797 Loss: 2.506544\nTraining Batch 361/797 Loss: 2.475549\nTraining Batch 362/797 Loss: 2.473196\nTraining Batch 363/797 Loss: 2.197830\nTraining Batch 364/797 Loss: 2.428666\nTraining Batch 365/797 Loss: 2.431653\nTraining Batch 366/797 Loss: 2.264620\nTraining Batch 367/797 Loss: 2.506172\nTraining Batch 368/797 Loss: 2.654270\nTraining Batch 369/797 Loss: 2.387779\nTraining Batch 370/797 Loss: 2.387083\nTraining Batch 371/797 Loss: 2.422066\nTraining Batch 372/797 Loss: 2.427381\nTraining Batch 373/797 Loss: 2.538957\nTraining Batch 374/797 Loss: 2.398950\nTraining Batch 375/797 Loss: 2.222907\nTraining Batch 376/797 Loss: 2.298057\nTraining Batch 377/797 Loss: 2.535975\nTraining Batch 378/797 Loss: 2.495114\nTraining Batch 379/797 Loss: 2.420651\nTraining Batch 380/797 Loss: 2.503584\nTraining Batch 381/797 Loss: 2.361018\nTraining Batch 382/797 Loss: 2.472056\nTraining Batch 383/797 Loss: 2.434513\nTraining Batch 384/797 Loss: 2.435919\nTraining Batch 385/797 Loss: 2.439738\nTraining Batch 386/797 Loss: 2.404214\nTraining Batch 387/797 Loss: 2.567045\nTraining Batch 388/797 Loss: 2.453310\nTraining Batch 389/797 Loss: 2.425217\nTraining Batch 390/797 Loss: 2.463032\nTraining Batch 391/797 Loss: 2.373708\nTraining Batch 392/797 Loss: 2.392803\nTraining Batch 393/797 Loss: 2.472723\nTraining Batch 394/797 Loss: 2.468834\nTraining Batch 395/797 Loss: 2.512482\nTraining Batch 396/797 Loss: 2.346033\nTraining Batch 397/797 Loss: 2.518518\nTraining Batch 398/797 Loss: 2.371109\nTraining Batch 399/797 Loss: 2.577794\nTraining Batch 400/797 Loss: 2.374159\nTraining Batch 401/797 Loss: 2.404648\nTraining Batch 402/797 Loss: 2.601397\nTraining Batch 403/797 Loss: 2.441407\nTraining Batch 404/797 Loss: 2.598015\nTraining Batch 405/797 Loss: 2.576705\nTraining Batch 406/797 Loss: 2.385991\nTraining Batch 407/797 Loss: 2.459372\nTraining Batch 408/797 Loss: 2.379482\nTraining Batch 409/797 Loss: 2.344119\nTraining Batch 410/797 Loss: 2.417860\nTraining Batch 411/797 Loss: 2.338934\nTraining Batch 412/797 Loss: 2.514755\nTraining Batch 413/797 Loss: 2.416236\nTraining Batch 414/797 Loss: 2.565289\nTraining Batch 415/797 Loss: 2.371418\nTraining Batch 416/797 Loss: 2.421891\nTraining Batch 417/797 Loss: 2.312799\nTraining Batch 418/797 Loss: 2.323928\nTraining Batch 419/797 Loss: 2.335836\nTraining Batch 420/797 Loss: 2.547111\nTraining Batch 421/797 Loss: 2.459069\nTraining Batch 422/797 Loss: 2.455011\nTraining Batch 423/797 Loss: 2.385337\nTraining Batch 424/797 Loss: 2.405946\nTraining Batch 425/797 Loss: 2.461591\nTraining Batch 426/797 Loss: 2.393987\nTraining Batch 427/797 Loss: 2.485054\nTraining Batch 428/797 Loss: 2.561065\nTraining Batch 429/797 Loss: 2.459322\nTraining Batch 430/797 Loss: 2.330632\nTraining Batch 431/797 Loss: 2.237527\nTraining Batch 432/797 Loss: 2.450076\nTraining Batch 433/797 Loss: 2.466329\nTraining Batch 434/797 Loss: 2.521106\nTraining Batch 435/797 Loss: 2.353661\nTraining Batch 436/797 Loss: 2.352907\nTraining Batch 437/797 Loss: 2.420199\nTraining Batch 438/797 Loss: 2.441821\nTraining Batch 439/797 Loss: 2.376431\nTraining Batch 440/797 Loss: 2.380746\nTraining Batch 441/797 Loss: 2.461828\nTraining Batch 442/797 Loss: 2.401907\nTraining Batch 443/797 Loss: 2.265032\nTraining Batch 444/797 Loss: 2.411769\nTraining Batch 445/797 Loss: 2.493733\nTraining Batch 446/797 Loss: 2.322628\nTraining Batch 447/797 Loss: 2.311789\nTraining Batch 448/797 Loss: 2.429433\nTraining Batch 449/797 Loss: 2.427613\nTraining Batch 450/797 Loss: 2.410613\nTraining Batch 451/797 Loss: 2.400406\nTraining Batch 452/797 Loss: 2.474858\nTraining Batch 453/797 Loss: 2.368865\nTraining Batch 454/797 Loss: 2.372939\nTraining Batch 455/797 Loss: 2.372865\nTraining Batch 456/797 Loss: 2.463751\nTraining Batch 457/797 Loss: 2.479528\nTraining Batch 458/797 Loss: 2.354007\nTraining Batch 459/797 Loss: 2.386793\nTraining Batch 460/797 Loss: 2.615830\nTraining Batch 461/797 Loss: 2.487552\nTraining Batch 462/797 Loss: 2.416334\nTraining Batch 463/797 Loss: 2.350457\nTraining Batch 464/797 Loss: 2.358334\nTraining Batch 465/797 Loss: 2.271719\nTraining Batch 466/797 Loss: 2.446264\nTraining Batch 467/797 Loss: 2.301311\nTraining Batch 468/797 Loss: 2.423393\nTraining Batch 469/797 Loss: 2.366363\nTraining Batch 470/797 Loss: 2.405754\nTraining Batch 471/797 Loss: 2.511120\nTraining Batch 472/797 Loss: 2.253040\nTraining Batch 473/797 Loss: 2.356257\nTraining Batch 474/797 Loss: 2.488003\nTraining Batch 475/797 Loss: 2.454806\nTraining Batch 476/797 Loss: 2.429765\nTraining Batch 477/797 Loss: 2.377102\nTraining Batch 478/797 Loss: 2.525764\nTraining Batch 479/797 Loss: 2.331855\nTraining Batch 480/797 Loss: 2.503651\nTraining Batch 481/797 Loss: 2.404991\nTraining Batch 482/797 Loss: 2.457541\nTraining Batch 483/797 Loss: 2.419297\nTraining Batch 484/797 Loss: 2.630526\nTraining Batch 485/797 Loss: 2.458864\nTraining Batch 486/797 Loss: 2.423454\nTraining Batch 487/797 Loss: 2.498753\nTraining Batch 488/797 Loss: 2.474880\nTraining Batch 489/797 Loss: 2.532216\nTraining Batch 490/797 Loss: 2.297150\nTraining Batch 491/797 Loss: 2.307464\nTraining Batch 492/797 Loss: 2.424164\nTraining Batch 493/797 Loss: 2.442307\nTraining Batch 494/797 Loss: 2.348745\nTraining Batch 495/797 Loss: 2.541717\nTraining Batch 496/797 Loss: 2.407046\nTraining Batch 497/797 Loss: 2.484701\nTraining Batch 498/797 Loss: 2.484851\nTraining Batch 499/797 Loss: 2.247781\nTraining Batch 500/797 Loss: 2.467655\nTraining Batch 501/797 Loss: 2.401849\nTraining Batch 502/797 Loss: 2.346670\nTraining Batch 503/797 Loss: 2.469132\nTraining Batch 504/797 Loss: 2.419828\nTraining Batch 505/797 Loss: 2.360834\nTraining Batch 506/797 Loss: 2.394572\nTraining Batch 507/797 Loss: 2.418855\nTraining Batch 508/797 Loss: 2.265215\nTraining Batch 509/797 Loss: 2.415143\nTraining Batch 510/797 Loss: 2.574696\nTraining Batch 511/797 Loss: 2.305452\nTraining Batch 512/797 Loss: 2.457211\nTraining Batch 513/797 Loss: 2.395114\nTraining Batch 514/797 Loss: 2.514512\nTraining Batch 515/797 Loss: 2.500952\nTraining Batch 516/797 Loss: 2.493386\nTraining Batch 517/797 Loss: 2.387729\nTraining Batch 518/797 Loss: 2.481068\nTraining Batch 519/797 Loss: 2.445774\nTraining Batch 520/797 Loss: 2.319551\nTraining Batch 521/797 Loss: 2.370307\nTraining Batch 522/797 Loss: 2.535177\nTraining Batch 523/797 Loss: 2.378651\nTraining Batch 524/797 Loss: 2.294051\nTraining Batch 525/797 Loss: 2.559840\nTraining Batch 526/797 Loss: 2.427751\nTraining Batch 527/797 Loss: 2.541047\nTraining Batch 528/797 Loss: 2.524277\nTraining Batch 529/797 Loss: 2.405365\nTraining Batch 530/797 Loss: 2.503463\nTraining Batch 531/797 Loss: 2.218800\nTraining Batch 532/797 Loss: 2.426305\nTraining Batch 533/797 Loss: 2.467250\nTraining Batch 534/797 Loss: 2.406738\nTraining Batch 535/797 Loss: 2.441585\nTraining Batch 536/797 Loss: 2.564943\nTraining Batch 537/797 Loss: 2.363654\nTraining Batch 538/797 Loss: 2.472219\nTraining Batch 539/797 Loss: 2.387609\nTraining Batch 540/797 Loss: 2.426403\nTraining Batch 541/797 Loss: 2.367001\nTraining Batch 542/797 Loss: 2.449524\nTraining Batch 543/797 Loss: 2.384790\nTraining Batch 544/797 Loss: 2.377491\nTraining Batch 545/797 Loss: 2.402377\nTraining Batch 546/797 Loss: 2.487552\nTraining Batch 547/797 Loss: 2.426938\nTraining Batch 548/797 Loss: 2.335036\nTraining Batch 549/797 Loss: 2.406309\nTraining Batch 550/797 Loss: 2.288236\nTraining Batch 551/797 Loss: 2.315035\nTraining Batch 552/797 Loss: 2.415251\nTraining Batch 553/797 Loss: 2.496129\nTraining Batch 554/797 Loss: 2.519070\nTraining Batch 555/797 Loss: 2.473168\nTraining Batch 556/797 Loss: 2.394539\nTraining Batch 557/797 Loss: 2.350425\nTraining Batch 558/797 Loss: 2.484581\nTraining Batch 559/797 Loss: 2.247961\nTraining Batch 560/797 Loss: 2.282632\nTraining Batch 561/797 Loss: 2.514509\nTraining Batch 562/797 Loss: 2.411076\nTraining Batch 563/797 Loss: 2.445015\nTraining Batch 564/797 Loss: 2.401559\nTraining Batch 565/797 Loss: 2.508302\nTraining Batch 566/797 Loss: 2.249797\nTraining Batch 567/797 Loss: 2.259727\nTraining Batch 568/797 Loss: 2.349664\nTraining Batch 569/797 Loss: 2.469649\nTraining Batch 570/797 Loss: 2.343263\nTraining Batch 571/797 Loss: 2.332633\nTraining Batch 572/797 Loss: 2.344903\nTraining Batch 573/797 Loss: 2.455399\nTraining Batch 574/797 Loss: 2.271165\nTraining Batch 575/797 Loss: 2.321492\nTraining Batch 576/797 Loss: 2.181935\nTraining Batch 577/797 Loss: 2.485657\nTraining Batch 578/797 Loss: 2.548748\nTraining Batch 579/797 Loss: 2.544624\nTraining Batch 580/797 Loss: 2.476452\nTraining Batch 581/797 Loss: 2.315025\nTraining Batch 582/797 Loss: 2.397038\nTraining Batch 583/797 Loss: 2.588297\nTraining Batch 584/797 Loss: 2.442238\nTraining Batch 585/797 Loss: 2.237530\nTraining Batch 586/797 Loss: 2.437910\nTraining Batch 587/797 Loss: 2.262236\nTraining Batch 588/797 Loss: 2.410773\nTraining Batch 589/797 Loss: 2.345532\nTraining Batch 590/797 Loss: 2.393255\nTraining Batch 591/797 Loss: 2.457475\nTraining Batch 592/797 Loss: 2.409694\nTraining Batch 593/797 Loss: 2.301888\nTraining Batch 594/797 Loss: 2.411911\nTraining Batch 595/797 Loss: 2.419695\nTraining Batch 596/797 Loss: 2.482850\nTraining Batch 597/797 Loss: 2.395507\nTraining Batch 598/797 Loss: 2.462354\nTraining Batch 599/797 Loss: 2.370898\nTraining Batch 600/797 Loss: 2.296421\nTraining Batch 601/797 Loss: 2.400728\nTraining Batch 602/797 Loss: 2.373450\nTraining Batch 603/797 Loss: 2.291641\nTraining Batch 604/797 Loss: 2.470114\nTraining Batch 605/797 Loss: 2.462953\nTraining Batch 606/797 Loss: 2.455031\nTraining Batch 607/797 Loss: 2.417222\nTraining Batch 608/797 Loss: 2.616131\nTraining Batch 609/797 Loss: 2.510682\nTraining Batch 610/797 Loss: 2.313320\nTraining Batch 611/797 Loss: 2.477776\nTraining Batch 612/797 Loss: 2.376407\nTraining Batch 613/797 Loss: 2.451502\nTraining Batch 614/797 Loss: 2.466384\nTraining Batch 615/797 Loss: 2.427359\nTraining Batch 616/797 Loss: 2.476893\nTraining Batch 617/797 Loss: 2.576396\nTraining Batch 618/797 Loss: 2.375241\nTraining Batch 619/797 Loss: 2.437123\nTraining Batch 620/797 Loss: 2.378016\nTraining Batch 621/797 Loss: 2.395018\nTraining Batch 622/797 Loss: 2.414740\nTraining Batch 623/797 Loss: 2.243799\nTraining Batch 624/797 Loss: 2.464196\nTraining Batch 625/797 Loss: 2.460088\nTraining Batch 626/797 Loss: 2.325431\nTraining Batch 627/797 Loss: 2.441413\nTraining Batch 628/797 Loss: 2.429297\nTraining Batch 629/797 Loss: 2.468429\nTraining Batch 630/797 Loss: 2.303580\nTraining Batch 631/797 Loss: 2.548331\nTraining Batch 632/797 Loss: 2.401847\nTraining Batch 633/797 Loss: 2.425178\nTraining Batch 634/797 Loss: 2.233671\nTraining Batch 635/797 Loss: 2.310937\nTraining Batch 636/797 Loss: 2.456881\nTraining Batch 637/797 Loss: 2.420142\nTraining Batch 638/797 Loss: 2.339876\nTraining Batch 639/797 Loss: 2.388821\nTraining Batch 640/797 Loss: 2.416126\nTraining Batch 641/797 Loss: 2.499714\nTraining Batch 642/797 Loss: 2.385842\nTraining Batch 643/797 Loss: 2.308710\nTraining Batch 644/797 Loss: 2.516577\nTraining Batch 645/797 Loss: 2.342716\nTraining Batch 646/797 Loss: 2.350397\nTraining Batch 647/797 Loss: 2.462233\nTraining Batch 648/797 Loss: 2.526313\nTraining Batch 649/797 Loss: 2.396167\nTraining Batch 650/797 Loss: 2.495421\nTraining Batch 651/797 Loss: 2.354813\nTraining Batch 652/797 Loss: 2.449403\nTraining Batch 653/797 Loss: 2.470673\nTraining Batch 654/797 Loss: 2.533539\nTraining Batch 655/797 Loss: 2.278406\nTraining Batch 656/797 Loss: 2.499719\nTraining Batch 657/797 Loss: 2.545443\nTraining Batch 658/797 Loss: 2.391034\nTraining Batch 659/797 Loss: 2.281662\nTraining Batch 660/797 Loss: 2.602639\nTraining Batch 661/797 Loss: 2.514074\nTraining Batch 662/797 Loss: 2.577498\nTraining Batch 663/797 Loss: 2.435680\nTraining Batch 664/797 Loss: 2.347803\nTraining Batch 665/797 Loss: 2.378120\nTraining Batch 666/797 Loss: 2.381301\nTraining Batch 667/797 Loss: 2.278649\nTraining Batch 668/797 Loss: 2.501714\nTraining Batch 669/797 Loss: 2.605863\nTraining Batch 670/797 Loss: 2.306761\nTraining Batch 671/797 Loss: 2.372409\nTraining Batch 672/797 Loss: 2.376897\nTraining Batch 673/797 Loss: 2.327114\nTraining Batch 674/797 Loss: 2.492284\nTraining Batch 675/797 Loss: 2.311509\nTraining Batch 676/797 Loss: 2.511337\nTraining Batch 677/797 Loss: 2.323014\nTraining Batch 678/797 Loss: 2.502828\nTraining Batch 679/797 Loss: 2.394134\nTraining Batch 680/797 Loss: 2.306421\nTraining Batch 681/797 Loss: 2.453323\nTraining Batch 682/797 Loss: 2.269292\nTraining Batch 683/797 Loss: 2.285691\nTraining Batch 684/797 Loss: 2.475200\nTraining Batch 685/797 Loss: 2.509812\nTraining Batch 686/797 Loss: 2.435467\nTraining Batch 687/797 Loss: 2.532319\nTraining Batch 688/797 Loss: 2.534463\nTraining Batch 689/797 Loss: 2.240462\nTraining Batch 690/797 Loss: 2.439247\nTraining Batch 691/797 Loss: 2.370683\nTraining Batch 692/797 Loss: 2.473148\nTraining Batch 693/797 Loss: 2.359055\nTraining Batch 694/797 Loss: 2.499702\nTraining Batch 695/797 Loss: 2.264504\nTraining Batch 696/797 Loss: 2.395367\nTraining Batch 697/797 Loss: 2.405915\nTraining Batch 698/797 Loss: 2.437767\nTraining Batch 699/797 Loss: 2.303988\nTraining Batch 700/797 Loss: 2.493806\nTraining Batch 701/797 Loss: 2.420563\nTraining Batch 702/797 Loss: 2.522291\nTraining Batch 703/797 Loss: 2.403799\nTraining Batch 704/797 Loss: 2.428790\nTraining Batch 705/797 Loss: 2.335932\nTraining Batch 706/797 Loss: 2.373153\nTraining Batch 707/797 Loss: 2.459358\nTraining Batch 708/797 Loss: 2.195248\nTraining Batch 709/797 Loss: 2.409977\nTraining Batch 710/797 Loss: 2.407955\nTraining Batch 711/797 Loss: 2.378067\nTraining Batch 712/797 Loss: 2.387476\nTraining Batch 713/797 Loss: 2.313925\nTraining Batch 714/797 Loss: 2.393331\nTraining Batch 715/797 Loss: 2.387585\nTraining Batch 716/797 Loss: 2.232840\nTraining Batch 717/797 Loss: 2.493126\nTraining Batch 718/797 Loss: 2.425428\nTraining Batch 719/797 Loss: 2.438835\nTraining Batch 720/797 Loss: 2.581184\nTraining Batch 721/797 Loss: 2.447954\nTraining Batch 722/797 Loss: 2.597685\nTraining Batch 723/797 Loss: 2.286386\nTraining Batch 724/797 Loss: 2.525974\nTraining Batch 725/797 Loss: 2.589593\nTraining Batch 726/797 Loss: 2.340477\nTraining Batch 727/797 Loss: 2.461375\nTraining Batch 728/797 Loss: 2.315192\nTraining Batch 729/797 Loss: 2.291127\nTraining Batch 730/797 Loss: 2.403030\nTraining Batch 731/797 Loss: 2.362506\nTraining Batch 732/797 Loss: 2.411582\nTraining Batch 733/797 Loss: 2.473128\nTraining Batch 734/797 Loss: 2.427561\nTraining Batch 735/797 Loss: 2.298818\nTraining Batch 736/797 Loss: 2.549217\nTraining Batch 737/797 Loss: 2.531517\nTraining Batch 738/797 Loss: 2.499990\nTraining Batch 739/797 Loss: 2.319603\nTraining Batch 740/797 Loss: 2.314835\nTraining Batch 741/797 Loss: 2.378911\nTraining Batch 742/797 Loss: 2.402154\nTraining Batch 743/797 Loss: 2.384935\nTraining Batch 744/797 Loss: 2.372134\nTraining Batch 745/797 Loss: 2.413939\nTraining Batch 746/797 Loss: 2.417875\nTraining Batch 747/797 Loss: 2.323245\nTraining Batch 748/797 Loss: 2.506105\nTraining Batch 749/797 Loss: 2.369904\nTraining Batch 750/797 Loss: 2.403642\nTraining Batch 751/797 Loss: 2.505374\nTraining Batch 752/797 Loss: 2.154368\nTraining Batch 753/797 Loss: 2.336820\nTraining Batch 754/797 Loss: 2.408010\nTraining Batch 755/797 Loss: 2.492727\nTraining Batch 756/797 Loss: 2.281012\nTraining Batch 757/797 Loss: 2.556116\nTraining Batch 758/797 Loss: 2.534687\nTraining Batch 759/797 Loss: 2.562189\nTraining Batch 760/797 Loss: 2.449984\nTraining Batch 761/797 Loss: 2.457741\nTraining Batch 762/797 Loss: 2.402183\nTraining Batch 763/797 Loss: 2.471616\nTraining Batch 764/797 Loss: 2.510933\nTraining Batch 765/797 Loss: 2.515812\nTraining Batch 766/797 Loss: 2.494937\nTraining Batch 767/797 Loss: 2.480112\nTraining Batch 768/797 Loss: 2.268251\nTraining Batch 769/797 Loss: 2.446028\nTraining Batch 770/797 Loss: 2.600976\nTraining Batch 771/797 Loss: 2.405744\nTraining Batch 772/797 Loss: 2.528074\nTraining Batch 773/797 Loss: 2.476164\nTraining Batch 774/797 Loss: 2.279891\nTraining Batch 775/797 Loss: 2.515243\nTraining Batch 776/797 Loss: 2.400885\nTraining Batch 777/797 Loss: 2.358558\nTraining Batch 778/797 Loss: 2.359676\nTraining Batch 779/797 Loss: 2.311880\nTraining Batch 780/797 Loss: 2.301580\nTraining Batch 781/797 Loss: 2.519713\nTraining Batch 782/797 Loss: 2.377555\nTraining Batch 783/797 Loss: 2.434333\nTraining Batch 784/797 Loss: 2.465950\nTraining Batch 785/797 Loss: 2.381703\nTraining Batch 786/797 Loss: 2.474564\nTraining Batch 787/797 Loss: 2.471446\nTraining Batch 788/797 Loss: 2.362590\nTraining Batch 789/797 Loss: 2.527132\nTraining Batch 790/797 Loss: 2.462375\nTraining Batch 791/797 Loss: 2.536442\nTraining Batch 792/797 Loss: 2.324486\nTraining Batch 793/797 Loss: 2.475605\nTraining Batch 794/797 Loss: 2.297320\nTraining Batch 795/797 Loss: 2.449361\nTraining Batch 796/797 Loss: 2.496888\n\nEpoch 2/10\nTraining Batch 0/797 Loss: 2.424233\nTraining Batch 1/797 Loss: 2.345075\nTraining Batch 2/797 Loss: 2.541657\nTraining Batch 3/797 Loss: 2.269932\nTraining Batch 4/797 Loss: 2.473559\nTraining Batch 5/797 Loss: 2.448077\nTraining Batch 6/797 Loss: 2.347411\nTraining Batch 7/797 Loss: 2.520365\nTraining Batch 8/797 Loss: 2.316255\nTraining Batch 9/797 Loss: 2.426029\nTraining Batch 10/797 Loss: 2.282500\nTraining Batch 11/797 Loss: 2.440804\nTraining Batch 12/797 Loss: 2.412784\nTraining Batch 13/797 Loss: 2.474339\nTraining Batch 14/797 Loss: 2.394248\nTraining Batch 15/797 Loss: 2.488020\nTraining Batch 16/797 Loss: 2.390137\nTraining Batch 17/797 Loss: 2.396797\nTraining Batch 18/797 Loss: 2.372271\nTraining Batch 19/797 Loss: 2.412142\nTraining Batch 20/797 Loss: 2.400482\nTraining Batch 21/797 Loss: 2.532327\nTraining Batch 22/797 Loss: 2.558636\nTraining Batch 23/797 Loss: 2.583962\nTraining Batch 24/797 Loss: 2.469394\nTraining Batch 25/797 Loss: 2.359146\nTraining Batch 26/797 Loss: 2.333361\nTraining Batch 27/797 Loss: 2.377175\nTraining Batch 28/797 Loss: 2.488229\nTraining Batch 29/797 Loss: 2.332640\nTraining Batch 30/797 Loss: 2.303284\nTraining Batch 31/797 Loss: 2.336591\nTraining Batch 32/797 Loss: 2.269327\nTraining Batch 33/797 Loss: 2.330159\nTraining Batch 34/797 Loss: 2.497517\nTraining Batch 35/797 Loss: 2.450471\nTraining Batch 36/797 Loss: 2.421362\nTraining Batch 37/797 Loss: 2.362644\nTraining Batch 38/797 Loss: 2.274938\nTraining Batch 39/797 Loss: 2.486109\nTraining Batch 40/797 Loss: 2.399517\nTraining Batch 41/797 Loss: 2.412964\nTraining Batch 42/797 Loss: 2.429776\nTraining Batch 43/797 Loss: 2.494770\nTraining Batch 44/797 Loss: 2.505741\nTraining Batch 45/797 Loss: 2.540360\nTraining Batch 46/797 Loss: 2.526602\nTraining Batch 47/797 Loss: 2.470413\nTraining Batch 48/797 Loss: 2.418425\nTraining Batch 49/797 Loss: 2.223494\nTraining Batch 50/797 Loss: 2.241713\nTraining Batch 51/797 Loss: 2.246255\nTraining Batch 52/797 Loss: 2.550171\nTraining Batch 53/797 Loss: 2.404784\nTraining Batch 54/797 Loss: 2.388589\nTraining Batch 55/797 Loss: 2.474234\nTraining Batch 56/797 Loss: 2.448174\nTraining Batch 57/797 Loss: 2.373706\nTraining Batch 58/797 Loss: 2.328531\nTraining Batch 59/797 Loss: 2.473605\nTraining Batch 60/797 Loss: 2.570797\nTraining Batch 61/797 Loss: 2.453926\nTraining Batch 62/797 Loss: 2.339896\nTraining Batch 63/797 Loss: 2.431259\nTraining Batch 64/797 Loss: 2.391235\nTraining Batch 65/797 Loss: 2.423646\nTraining Batch 66/797 Loss: 2.442633\nTraining Batch 67/797 Loss: 2.470963\nTraining Batch 68/797 Loss: 2.563621\nTraining Batch 69/797 Loss: 2.506472\nTraining Batch 70/797 Loss: 2.580923\nTraining Batch 71/797 Loss: 2.683894\nTraining Batch 72/797 Loss: 2.596423\nTraining Batch 73/797 Loss: 2.418120\nTraining Batch 74/797 Loss: 2.441932\nTraining Batch 75/797 Loss: 2.468868\nTraining Batch 76/797 Loss: 2.257462\nTraining Batch 77/797 Loss: 2.454709\nTraining Batch 78/797 Loss: 2.445814\nTraining Batch 79/797 Loss: 2.498039\nTraining Batch 80/797 Loss: 2.387996\nTraining Batch 81/797 Loss: 2.497314\nTraining Batch 82/797 Loss: 2.404615\nTraining Batch 83/797 Loss: 2.483459\nTraining Batch 84/797 Loss: 2.384223\nTraining Batch 85/797 Loss: 2.386803\nTraining Batch 86/797 Loss: 2.454023\nTraining Batch 87/797 Loss: 2.463098\nTraining Batch 88/797 Loss: 2.352285\nTraining Batch 89/797 Loss: 2.436855\nTraining Batch 90/797 Loss: 2.502198\nTraining Batch 91/797 Loss: 2.455320\nTraining Batch 92/797 Loss: 2.312085\nTraining Batch 93/797 Loss: 2.562878\nTraining Batch 94/797 Loss: 2.515559\nTraining Batch 95/797 Loss: 2.381211\nTraining Batch 96/797 Loss: 2.435347\nTraining Batch 97/797 Loss: 2.384759\nTraining Batch 98/797 Loss: 2.502372\nTraining Batch 99/797 Loss: 2.341605\nTraining Batch 100/797 Loss: 2.493334\nTraining Batch 101/797 Loss: 2.321497\nTraining Batch 102/797 Loss: 2.332486\nTraining Batch 103/797 Loss: 2.363564\nTraining Batch 104/797 Loss: 2.278440\nTraining Batch 105/797 Loss: 2.355561\nTraining Batch 106/797 Loss: 2.431717\nTraining Batch 107/797 Loss: 2.420067\nTraining Batch 108/797 Loss: 2.501751\nTraining Batch 109/797 Loss: 2.589463\nTraining Batch 110/797 Loss: 2.424767\nTraining Batch 111/797 Loss: 2.402086\nTraining Batch 112/797 Loss: 2.492729\nTraining Batch 113/797 Loss: 2.358969\nTraining Batch 114/797 Loss: 2.497308\nTraining Batch 115/797 Loss: 2.550787\nTraining Batch 116/797 Loss: 2.466899\nTraining Batch 117/797 Loss: 2.449072\nTraining Batch 118/797 Loss: 2.432943\nTraining Batch 119/797 Loss: 2.553330\nTraining Batch 120/797 Loss: 2.351972\nTraining Batch 121/797 Loss: 2.239412\nTraining Batch 122/797 Loss: 2.383634\nTraining Batch 123/797 Loss: 2.442912\nTraining Batch 124/797 Loss: 2.555036\nTraining Batch 125/797 Loss: 2.514959\nTraining Batch 126/797 Loss: 2.347295\nTraining Batch 127/797 Loss: 2.451676\nTraining Batch 128/797 Loss: 2.414452\nTraining Batch 129/797 Loss: 2.481751\nTraining Batch 130/797 Loss: 2.463321\nTraining Batch 131/797 Loss: 2.564465\nTraining Batch 132/797 Loss: 2.559389\nTraining Batch 133/797 Loss: 2.531281\nTraining Batch 134/797 Loss: 2.433643\nTraining Batch 135/797 Loss: 2.315119\nTraining Batch 136/797 Loss: 2.448251\nTraining Batch 137/797 Loss: 2.379290\nTraining Batch 138/797 Loss: 2.364897\nTraining Batch 139/797 Loss: 2.348393\nTraining Batch 140/797 Loss: 2.316560\nTraining Batch 141/797 Loss: 2.512531\nTraining Batch 142/797 Loss: 2.541739\nTraining Batch 143/797 Loss: 2.362395\nTraining Batch 144/797 Loss: 2.344354\nTraining Batch 145/797 Loss: 2.358912\nTraining Batch 146/797 Loss: 2.249277\nTraining Batch 147/797 Loss: 2.554012\nTraining Batch 148/797 Loss: 2.457289\nTraining Batch 149/797 Loss: 2.486684\nTraining Batch 150/797 Loss: 2.555213\nTraining Batch 151/797 Loss: 2.533917\nTraining Batch 152/797 Loss: 2.410015\nTraining Batch 153/797 Loss: 2.641131\nTraining Batch 154/797 Loss: 2.330610\nTraining Batch 155/797 Loss: 2.344919\nTraining Batch 156/797 Loss: 2.517002\nTraining Batch 157/797 Loss: 2.331555\nTraining Batch 158/797 Loss: 2.505350\nTraining Batch 159/797 Loss: 2.361831\nTraining Batch 160/797 Loss: 2.587157\nTraining Batch 161/797 Loss: 2.393278\nTraining Batch 162/797 Loss: 2.459169\nTraining Batch 163/797 Loss: 2.689083\nTraining Batch 164/797 Loss: 2.408891\nTraining Batch 165/797 Loss: 2.439810\nTraining Batch 166/797 Loss: 2.498642\nTraining Batch 167/797 Loss: 2.458311\nTraining Batch 168/797 Loss: 2.592261\nTraining Batch 169/797 Loss: 2.276734\nTraining Batch 170/797 Loss: 2.316135\nTraining Batch 171/797 Loss: 2.456622\nTraining Batch 172/797 Loss: 2.304363\nTraining Batch 173/797 Loss: 2.480429\nTraining Batch 174/797 Loss: 2.436170\nTraining Batch 175/797 Loss: 2.443254\nTraining Batch 176/797 Loss: 2.354367\nTraining Batch 177/797 Loss: 2.336644\nTraining Batch 178/797 Loss: 2.424558\nTraining Batch 179/797 Loss: 2.548441\nTraining Batch 180/797 Loss: 2.433899\nTraining Batch 181/797 Loss: 2.625690\nTraining Batch 182/797 Loss: 2.446163\nTraining Batch 183/797 Loss: 2.330786\nTraining Batch 184/797 Loss: 2.411632\nTraining Batch 185/797 Loss: 2.347468\nTraining Batch 186/797 Loss: 2.393611\nTraining Batch 187/797 Loss: 2.364631\nTraining Batch 188/797 Loss: 2.397482\nTraining Batch 189/797 Loss: 2.387913\nTraining Batch 190/797 Loss: 2.352729\nTraining Batch 191/797 Loss: 2.476573\nTraining Batch 192/797 Loss: 2.343880\nTraining Batch 193/797 Loss: 2.564952\nTraining Batch 194/797 Loss: 2.373493\nTraining Batch 195/797 Loss: 2.469547\nTraining Batch 196/797 Loss: 2.437370\nTraining Batch 197/797 Loss: 2.423698\nTraining Batch 198/797 Loss: 2.412416\nTraining Batch 199/797 Loss: 2.455674\nTraining Batch 200/797 Loss: 2.424251\nTraining Batch 201/797 Loss: 2.542900\nTraining Batch 202/797 Loss: 2.318745\nTraining Batch 203/797 Loss: 2.491597\nTraining Batch 204/797 Loss: 2.503101\nTraining Batch 205/797 Loss: 2.546688\nTraining Batch 206/797 Loss: 2.280017\nTraining Batch 207/797 Loss: 2.357542\nTraining Batch 208/797 Loss: 2.554121\nTraining Batch 209/797 Loss: 2.327704\nTraining Batch 210/797 Loss: 2.628652\nTraining Batch 211/797 Loss: 2.455050\nTraining Batch 212/797 Loss: 2.420997\nTraining Batch 213/797 Loss: 2.542747\nTraining Batch 214/797 Loss: 2.403507\nTraining Batch 215/797 Loss: 2.399814\nTraining Batch 216/797 Loss: 2.571538\nTraining Batch 217/797 Loss: 2.395319\nTraining Batch 218/797 Loss: 2.305351\nTraining Batch 219/797 Loss: 2.399097\nTraining Batch 220/797 Loss: 2.328417\nTraining Batch 221/797 Loss: 2.379441\nTraining Batch 222/797 Loss: 2.470899\nTraining Batch 223/797 Loss: 2.330586\nTraining Batch 224/797 Loss: 2.411530\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m trn_corr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X_train, y_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X_train\u001b[38;5;241m.\u001b[39mto(device))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36mAudioMNISTDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m         padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio[idx])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#         print(padded.size())\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m         audio_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmfcc_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#         print(audio_seq.size())\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         normalized_data \u001b[38;5;241m=\u001b[39m (audio_seq \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m))\n","Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mAudioMNISTDataset.mfcc_data\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmfcc_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, file):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         print(file)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m         spectrogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToSpectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m         spectrogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mToDB(spectrogram)\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m spectrogram[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:650\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    643\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m     specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/functional/functional.py:126\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    123\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_length_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# unpack batch\u001b[39;00m\n\u001b[1;32m    140\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mreshape(shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/functional.py:641\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    642\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}